{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8310555,"sourceType":"datasetVersion","datasetId":4936789}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nmodel_name = 'FacebookAI/roberta-base'\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, num_labels=4)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4)\n\n# Define device (CPU or GPU)\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice = 'cpu'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ntrain_url = '/kaggle/input/exist-2024-tweets-dataset/EXIST 2024 Tweets Dataset/training/EXIST2024_training.json'\nval_url = '/kaggle/input/exist-2024-tweets-dataset/EXIST 2024 Tweets Dataset/dev/EXIST2024_dev.json'\ntest_url = '/kaggle/input/exist-2024-tweets-dataset/EXIST 2024 Tweets Dataset/test/EXIST2023_test_clean.json'\n\ndf_train = pd.read_json(train_url)\ndf_val = pd.read_json(val_url)\ndf_test = pd.read_json(test_url)\n\ndf_train = df_train.transpose()\ndf_val = df_val.transpose()\ndf_test = df_test.transpose()\n\ndf_train['id_EXIST'] = df_train['id_EXIST'].astype(int)\ndf_val['id_EXIST'] = df_val['id_EXIST'].astype(int)\n\ntrain_labels = pd.read_json('/kaggle/input/exist-2024-tweets-dataset/golds/EXIST2024_training_task2_gold_hard.json')\nval_labels = pd.read_json('/kaggle/input/exist-2024-tweets-dataset/golds/EXIST2024_dev_task2_gold_hard.json')\n\ndf_train = pd.merge(df_train,train_labels, left_on='id_EXIST' ,right_on='id' , how='inner')\ndf_val = pd.merge(df_val,val_labels, left_on='id_EXIST' ,right_on='id' , how='inner')\n\ndf_train['value'] = df_train['value'].astype(str)\ndf_val['value'] = df_val['value'].astype(str)\n\ndf_train.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['Label'] = 0\ndf_val['Label'] = 0\n\ndef Labeling(df):\n    for index, row in df.iterrows():\n        labels = row['value']\n        if labels == 'DIRECT':\n            df.at[index, 'Label'] = 0 \n        elif labels == 'REPORTED':\n            df.at[index, 'Label'] = 1\n        elif labels == 'JUDGEMENTAL' :\n            df.at[index, 'Label'] = 2\n        else:\n            df.at[index, 'Label'] = 3\n    return df\n\n\ndf_train = Labeling(df_train)\ndf_val= Labeling(df_val)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_no = 5\n\n# Prepare the training data\ntrain_texts = df_train['tweet'].tolist()\ntrain_labels = df_train['Label'].tolist()\n\nval_texts = df_val['tweet'].tolist()\nval_labels = df_val['Label'].tolist()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tokenize and encode the training texts\ntrain_encodings = tokenizer(train_texts, truncation=True, max_length=128,\n        padding='max_length', return_tensors = 'pt')\n\n# Convert the labels to tensors\ntrain_labels =torch.tensor(train_labels)\n\n\n# Create a PyTorch dataset\ntrain_dataset = torch.utils.data.TensorDataset(train_encodings['input_ids'],\n                                               train_encodings['attention_mask'],\n                                               train_labels)\n\n# Create a data loader\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n\n\n\nmodel = model.to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tqdm.notebook import tqdm\n\n# for batch in tqdm(train_loader):\n\n#     print(f\"batch_0 {batch[0]} batch_1 {batch[1]} batch_2 {batch[2]}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.notebook import tqdm\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nfrom sklearn.metrics import accuracy_score\n\n# Set the model to training mode\nmodel.train()\n\n# Define the optimizer and loss function\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\nloss_fn = torch.nn.CrossEntropyLoss()\n\nlosses = []\naccuracies = []  # To store accuracy per epoch\nnum_epochs = 5 #changed to 7\n# Training loop\nfor epoch in tqdm(range(num_epochs)):  # Number of training epochs\n    running_loss = 0.0\n    predicted_labels = []  # To store predicted labels for accuracy calculation\n    true_labels = []  # To store true labels for accuracy calculation\n\n    for batch in tqdm(train_loader):\n        \n#         print(f\"Max label: {labels.max().item()}, Number of classes: {4}\")\n        \n#         assert labels.max().item() < 4, \"An index in labels is out of range\"\n        \n        input_ids = batch[0].to(device)\n        attention_mask = batch[1].to(device)\n        labels =  batch[2].to(device) \n        \n        optimizer.zero_grad()\n\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        logits = outputs.logits\n\n        loss.backward()\n#         optimizer.step()\n\n        running_loss += loss.item()\n\n        # Convert logits to predicted labels\n        _, predicted = torch.max(logits, dim=1)\n        predicted_labels.extend(predicted.cpu().tolist())\n        true_labels.extend(labels.cpu().tolist())\n\n    epoch_loss = running_loss / len(train_loader)\n    losses.append(epoch_loss)\n\n    # Calculate and store accuracy\n    accuracy = accuracy_score(true_labels, predicted_labels)\n    accuracies.append(accuracy)\n\n    print(f'Epoch {epoch + 1}/{num_epochs} - Loss: {epoch_loss:.4f} - Accuracy: {accuracy:.4f}')\n\n# Save the model\ntorch.save(model.state_dict(), 'model.pth')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n\ndef predict_labels(text):\n    train_encodings = tokenizer(text, truncation=True, max_length=128,\n        padding='max_length', return_tensors = 'pt')\n    input_ids = train_encodings['input_ids'].to(device)\n    attention_mask = train_encodings['attention_mask'].to(device)\n\n    # Set the model to evaluation mode\n    model.eval()\n\n    # Disable gradient calculation\n    with torch.no_grad():\n        outputs = model(input_ids, attention_mask=attention_mask)\n\n    logits = outputs.logits\n    probabilities = torch.softmax(logits, dim=1)\n    predicted_class = torch.argmax(probabilities, dim=1)\n    \n\n    return predicted_class.item(), probabilities[:,1].item()\npredicted_labels = []\npredicted_probs = []\nfor text in tqdm(val_texts):\n    predicted_label, prob = predict_labels(text)\n    predicted_labels.append(predicted_label)\n    predicted_probs.append(prob)\n\n# Calculate accuracy and F1 score\naccuracy = accuracy_score(val_labels, predicted_labels)\n# f1 = f1_score(valid_labels, predicted_labels)\n# roc_auc = roc_auc_score(valid_labels, predicted_probs)\n\nprint('Accuracy:', accuracy)\n# print('F1 Score:', f1)'xlm-roberta-base'\n# print('ROC-AUC:', roc_auc)\nfrom sklearn.metrics import roc_auc_score, classification_report\nprint(model_name + f\"    epoch = {num_epochs}\")\nprint('\\nThe Classification Report is as follows\\n')\nprint(classification_report(val_labels, predicted_labels, digits = 4))","metadata":{},"execution_count":null,"outputs":[]}]}